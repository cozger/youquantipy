#!/usr/bin/env python3
"""
Visual test for the GPU pipeline with face mesh overlay and performance metrics.
Tests the complete pipeline from camera capture through detection and landmark processing.

This test script:
1. Starts a CameraWorker process with GPU pipeline
2. Connects to shared memory buffers for preview frames and landmarks
3. Displays live video with face mesh overlay
4. Shows real-time performance metrics on screen
5. Supports keyboard controls (q=quit, p=pause, s=screenshot)

Usage:
    python test_gpu_pipeline_visual.py --camera 0 --duration 60
"""

import cv2
import numpy as np
import time
import multiprocessing as mp
from multiprocessing import Process, Queue, Pipe, shared_memory
import signal
import sys
import logging
from collections import deque
from dataclasses import dataclass
from typing import Optional, List, Tuple, Dict

# Import our modules
from camera_worker import CameraWorker

# Configure logging
logging.basicConfig(
    level=logging.DEBUG,  # Enable DEBUG level to see landmark processing details
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('GPUPipelineVisualTest')

# MediaPipe Face Mesh Connections (FACEMESH_CONTOURS)
# Generated by extract_mediapipe_connections.py - hardcoded to avoid mediapipe dependency
FACE_CONNECTIONS = [
    (270, 409), (176, 149), (37, 0), (84, 17), (318, 324), (293, 334), (386, 385), (7, 163),
    (33, 246), (17, 314), (374, 380), (251, 389), (390, 373), (267, 269), (295, 285), (389, 356),
    (173, 133), (33, 7), (377, 152), (158, 157), (405, 321), (54, 103), (263, 466), (324, 308),
    (67, 109), (409, 291), (157, 173), (454, 323), (388, 387), (78, 191), (148, 176), (311, 310),
    (39, 37), (249, 390), (144, 145), (402, 318), (80, 81), (310, 415), (153, 154), (384, 398),
    (397, 365), (234, 127), (103, 67), (282, 295), (338, 297), (378, 400), (127, 162), (321, 375),
    (375, 291), (317, 402), (81, 82), (154, 155), (91, 181), (334, 296), (297, 332), (269, 270),
    (150, 136), (109, 10), (356, 454), (58, 132), (312, 311), (152, 148), (415, 308), (161, 160),
    (296, 336), (65, 55), (61, 146), (78, 95), (380, 381), (398, 362), (361, 288), (246, 161),
    (162, 21), (0, 267), (82, 13), (132, 93), (314, 405), (10, 338), (178, 87), (387, 386),
    (381, 382), (70, 63), (61, 185), (14, 317), (105, 66), (300, 293), (382, 362), (88, 178),
    (185, 40), (46, 53), (284, 251), (400, 377), (136, 172), (323, 361), (13, 312), (21, 54),
    (172, 58), (373, 374), (163, 144), (276, 283), (53, 52), (365, 379), (379, 378), (146, 91),
    (263, 249), (283, 282), (87, 14), (145, 153), (155, 133), (93, 234), (66, 107), (95, 88),
    (159, 158), (52, 65), (332, 284), (40, 39), (191, 80), (63, 105), (181, 84), (466, 388),
    (149, 150), (288, 397), (160, 159), (385, 384)
]

logger.info(f"Using hardcoded MediaPipe face mesh connections ({len(FACE_CONNECTIONS)} connections)")

@dataclass
class PerformanceMetrics:
    """Container for performance tracking"""
    fps: float = 0.0
    frame_time_ms: float = 0.0
    detection_time_ms: float = 0.0
    landmark_time_ms: float = 0.0
    draw_time_ms: float = 0.0
    total_frames: int = 0
    detected_faces: int = 0
    
    # Rolling averages
    fps_history: deque = None
    frame_time_history: deque = None
    detection_time_history: deque = None
    landmark_time_history: deque = None
    
    def __post_init__(self):
        self.fps_history = deque(maxlen=30)
        self.frame_time_history = deque(maxlen=30)
        self.detection_time_history = deque(maxlen=30)
        self.landmark_time_history = deque(maxlen=30)
    
    def update_averages(self):
        """Update rolling averages"""
        if self.fps_history:
            self.fps = np.mean(self.fps_history)
        if self.frame_time_history:
            self.frame_time_ms = np.mean(self.frame_time_history)
        if self.detection_time_history:
            self.detection_time_ms = np.mean(self.detection_time_history)
        if self.landmark_time_history:
            self.landmark_time_ms = np.mean(self.landmark_time_history)

class GPUPipelineVisualTest:
    def __init__(self, camera_id: int = 0, target_resolution: List[int] = [1920, 1080], 
                 target_fps: int = 30):
        self.camera_id = camera_id
        self.target_resolution = target_resolution
        self.target_fps = target_fps
        self.running = False
        
        # Performance tracking
        self.metrics = PerformanceMetrics()
        self.last_frame_time = time.time()
        
        # Shared memory handles
        self.preview_shm = None
        self.landmark_shm = None
        
        # Display window
        self.window_name = f"GPU Pipeline Visual Test - Camera {camera_id}"
        
        # Stabilization tracking
        self.landmark_history = {}  # {face_idx: deque of landmarks}
        self.landmark_smoothing_window = 7  # Number of frames to average (reduced to minimize lag)
        self.stability_threshold = 10.0  # Pixel distance threshold for stability (increased to reduce jitter)
        
    def cleanup(self):
        """Clean up resources"""
        try:
            if self.preview_shm:
                self.preview_shm.close()
            if self.landmark_shm:
                self.landmark_shm.close()
            cv2.destroyAllWindows()
        except Exception as e:
            logger.error(f"Cleanup error: {e}")
    
    def stabilize_landmarks(self, face_idx: int, landmarks: np.ndarray) -> np.ndarray:
        """Apply aggressive temporal smoothing to landmarks to reduce jitter"""
        if face_idx not in self.landmark_history:
            self.landmark_history[face_idx] = {
                'frames': deque(maxlen=self.landmark_smoothing_window),
                'velocity': np.zeros_like(landmarks),
                'prediction': landmarks.copy()
            }
        
        history = self.landmark_history[face_idx]
        
        # Add current landmarks to history
        history['frames'].append(landmarks.copy())
        
        # If we don't have enough history, return current landmarks with minimal smoothing
        if len(history['frames']) < 3:
            return landmarks
        
        # Get frame history
        frames = list(history['frames'])
        
        # Calculate exponential moving average with more weight on recent frames
        weights = np.exp(np.linspace(-2, 0, len(frames)))
        weights /= np.sum(weights)
        
        # Weighted average
        smoothed = np.zeros_like(landmarks)
        for i, frame in enumerate(frames):
            smoothed += weights[i] * frame
        
        # Calculate velocity-based prediction
        if len(frames) >= 3:
            # Estimate velocity from last 3 frames
            velocity = (frames[-1] - frames[-3]) / 2.0
            # Dampen velocity to prevent oscillation (reduced for better responsiveness)
            velocity *= 0.3
            history['velocity'] = 0.8 * history['velocity'] + 0.2 * velocity
            
            # Predict next position
            predicted = frames[-1] + history['velocity']
            
            # Blend prediction with smoothed result
            smoothed = 0.7 * smoothed + 0.3 * predicted
        
        # Apply additional outlier rejection
        if len(frames) >= 2:
            prev_landmarks = frames[-2]
            current_landmarks = frames[-1]
            
            # Calculate per-landmark movement
            movement = np.linalg.norm(current_landmarks[:, :2] - prev_landmarks[:, :2], axis=1)
            
            # Identify outliers (landmarks that moved too much)
            outlier_threshold = np.percentile(movement, 90) + 2 * self.stability_threshold
            outliers = movement > outlier_threshold
            
            # For outliers, use more aggressive smoothing
            for i in range(landmarks.shape[0]):
                if outliers[i]:
                    # Use heavily weighted average for outliers
                    smoothed[i] = 0.05 * landmarks[i] + 0.95 * smoothed[i]
                else:
                    # Normal smoothing for stable landmarks
                    smoothed[i] = 0.2 * landmarks[i] + 0.8 * smoothed[i]
        
        history['prediction'] = smoothed.copy()
        
        return smoothed.astype(landmarks.dtype)
    
    def draw_face_mesh(self, image: np.ndarray, landmarks: np.ndarray, 
                      color: Tuple[int, int, int] = (0, 255, 0)) -> float:
        """Draw face mesh on image and return draw time"""
        start_time = time.time()
        
        if landmarks.shape[0] < 468:
            return 0.0
        
        h, w = image.shape[:2]
        
        # Draw connections with bounds checking
        for connection in FACE_CONNECTIONS:
            idx1, idx2 = connection[0], connection[1]
            if idx1 < landmarks.shape[0] and idx2 < landmarks.shape[0]:
                pt1 = landmarks[idx1][:2].astype(int)
                pt2 = landmarks[idx2][:2].astype(int)
                
                # Bounds check
                if (0 <= pt1[0] < w and 0 <= pt1[1] < h and 
                    0 <= pt2[0] < w and 0 <= pt2[1] < h):
                    cv2.line(image, tuple(pt1), tuple(pt2), color, 1, cv2.LINE_AA)
        
        # Draw key points with bounds checking (optional, can be commented out for cleaner look)
        for i in range(min(468, landmarks.shape[0])):
            pt = landmarks[i][:2].astype(int)
            if 0 <= pt[0] < w and 0 <= pt[1] < h:
                cv2.circle(image, tuple(pt), 1, (255, 255, 255), -1)
        
        return (time.time() - start_time) * 1000
    
    def draw_metrics(self, image: np.ndarray):
        """Draw performance metrics on image"""
        # Create semi-transparent overlay for text background
        overlay = image.copy()
        cv2.rectangle(overlay, (10, 10), (400, 200), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, image, 0.3, 0, image)
        
        # Draw metrics
        y_offset = 30
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        thickness = 1
        color = (0, 255, 0)
        
        texts = [
            f"FPS: {self.metrics.fps:.1f}",
            f"Frame Time: {self.metrics.frame_time_ms:.1f}ms",
            f"Detection: {self.metrics.detection_time_ms:.1f}ms",
            f"Landmarks: {self.metrics.landmark_time_ms:.1f}ms",
            f"Draw Time: {self.metrics.draw_time_ms:.1f}ms",
            f"Total Frames: {self.metrics.total_frames}",
            f"Faces Detected: {self.metrics.detected_faces}"
        ]
        
        for text in texts:
            cv2.putText(image, text, (20, y_offset), font, font_scale, color, thickness)
            y_offset += 25
    
    def run(self, duration: int = 60):
        """Run the visual test for specified duration"""
        logger.info(f"Starting GPU pipeline visual test for {duration} seconds")
        
        # Load configuration
        from confighandler import ConfigHandler
        config = ConfigHandler().config
        
        # Create communication queues
        control_queue = mp.Queue()
        status_queue = mp.Queue()
        metadata_queue = mp.Queue()
        
        # Start camera worker
        camera_worker = CameraWorker(
            camera_index=self.camera_id,
            gpu_device_id=0,
            config=config,
            control_queue=control_queue,
            status_queue=status_queue,
            metadata_queue=metadata_queue
        )
        
        camera_worker.start()
        logger.info(f"Camera worker process started (PID: {camera_worker.pid})")
        
        try:
            # Wait for initialization
            shared_memory_names = None
            resolution = None
            timeout_start = time.time()
            
            while time.time() - timeout_start < 10:  # 10 second timeout
                while not status_queue.empty():
                    try:
                        status = status_queue.get_nowait()
                        if status['type'] == 'ready':
                            resolution = status['data']['resolution']
                            shared_memory_names = status['data']['shared_memory']
                            logger.info(f"Camera ready! Resolution: {resolution}")
                            break
                    except:
                        pass
                
                if shared_memory_names:
                    break
                time.sleep(0.1)
            
            if not shared_memory_names:
                raise RuntimeError("Camera failed to initialize within timeout")
            
            # Metadata structure (define early for size calculations)
            metadata_dtype = np.dtype([
                ('frame_id', 'int32'),
                ('timestamp_ms', 'int64'),
                ('n_faces', 'int32'),
                ('ready', 'int8'),
            ])
            
            # Connect to shared memory
            preview_name = shared_memory_names['preview']
            landmark_name = shared_memory_names['landmarks']
            
            # Preview size is fixed in gpu_pipeline.py config
            preview_height, preview_width  = 960, 540
            preview_resolution = (preview_width, preview_height)
            preview_size = preview_width * preview_height * 3
            self.preview_shm = shared_memory.SharedMemory(name=preview_name)
            logger.info(f"Connected to preview shared memory: {preview_name}, size: {self.preview_shm.size}")
            
            # Match the actual size from gpu_pipeline.py
            max_faces = 10
            landmark_points = 478
            landmark_data_size = max_faces * landmark_points * 3 * 4  # float32
            landmark_buffer_size = landmark_data_size + metadata_dtype.itemsize
            
            self.landmark_shm = shared_memory.SharedMemory(name=landmark_name)
            logger.info(f"Connected to landmark shared memory: {landmark_name}, size: {self.landmark_shm.size}")
            
            # Log expected vs actual sizes (shared memory rounds up to page boundaries)
            expected_preview_size = preview_size + metadata_dtype.itemsize
            logger.info(f"Preview buffer - Expected: {expected_preview_size}, Actual: {self.preview_shm.size} (page-aligned)")
            logger.info(f"Landmark buffer - Expected: {landmark_buffer_size}, Actual: {self.landmark_shm.size} (page-aligned)")
            logger.info(f"Preview resolution: {preview_resolution}, size per frame: {preview_size} bytes")
            
            # Create display window
            cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)
            cv2.resizeWindow(self.window_name, 1280, 720)
            
            # Main display loop
            self.running = True
            start_time = time.time()
            frame_count = 0
            
            while self.running and (time.time() - start_time) < duration:
                loop_start = time.time()
                
                # Process metadata queue for performance info
                while not metadata_queue.empty():
                    try:
                        metadata = metadata_queue.get_nowait()
                        self.metrics.detection_time_history.append(metadata.get('processing_time_ms', 0))
                        self.metrics.detected_faces = metadata.get('n_faces', 0)
                    except:
                        pass
                
                # Get frame from shared memory with metadata
                preview_metadata_size = metadata_dtype.itemsize
                preview_buffer_size = preview_size + preview_metadata_size
                
                # Read metadata first - ensure buffer is large enough
                try:
                    metadata_buffer = self.preview_shm.buf[preview_size:preview_buffer_size]
                    if len(metadata_buffer) >= preview_metadata_size:
                        preview_metadata = np.frombuffer(metadata_buffer, dtype=metadata_dtype)[0]
                    else:
                        # Buffer too small, skip this iteration
                        logger.warning(f"Preview metadata buffer too small: {len(metadata_buffer)} < {preview_metadata_size}")
                        time.sleep(0.001)
                        continue
                except Exception as e:
                    logger.error(f"Error reading preview metadata: {e}")
                    time.sleep(0.001)
                    continue
                
                # Check if frame is ready
                if preview_metadata['ready'] == 1:
                    # Get frame from shared memory - numpy arrays are (height, width, channels)
                    frame = np.frombuffer(self.preview_shm.buf[:preview_size], 
                                        dtype=np.uint8).reshape(preview_height, preview_width, 3)
                    
                    # Resize to display resolution if needed
                    if (preview_width, preview_height) != (resolution[0], resolution[1]):
                        display_frame = cv2.resize(frame, (resolution[0], resolution[1]))
                    else:
                        display_frame = frame.copy()
                    
                    # Mark frame as read
                    preview_metadata['ready'] = 0
                else:
                    # Skip this iteration if no new frame
                    time.sleep(0.001)
                    continue
                
                # Get landmarks from shared memory
                max_faces = 10
                landmark_points = 478
                landmark_size = max_faces * landmark_points * 3 * 4  # float32
                
                # Read landmark metadata
                landmark_metadata_buffer = self.landmark_shm.buf[landmark_size:landmark_size + metadata_dtype.itemsize]
                landmark_metadata = np.frombuffer(landmark_metadata_buffer, dtype=metadata_dtype)[0]
                
                if landmark_metadata['ready'] == 1 and landmark_metadata['frame_id'] == preview_metadata['frame_id']:
                    # Create landmark array view
                    landmark_array = np.ndarray(
                        (max_faces, landmark_points, 3),
                        dtype=np.float32,
                        buffer=self.landmark_shm.buf[:landmark_size]
                    )
                    
                    num_faces = landmark_metadata['n_faces']
                    logger.debug(f"DEBUG: Landmark metadata - ready: {landmark_metadata['ready']}, frame_id: {landmark_metadata['frame_id']}, n_faces: {num_faces}")
                    
                    if num_faces > 0:
                        self.metrics.detected_faces = num_faces
                        logger.debug(f"DEBUG: Processing landmarks for {num_faces} faces")
                        
                        # Process each face's landmarks
                        for face_idx in range(min(num_faces, max_faces)):
                            landmarks = landmark_array[face_idx]
                            
                            # Debug landmark data
                            landmarks_nonzero = np.count_nonzero(landmarks)
                            landmarks_min = np.min(landmarks[landmarks > 0]) if landmarks_nonzero > 0 else 0
                            landmarks_max = np.max(landmarks) if landmarks_nonzero > 0 else 0
                            logger.debug(f"DEBUG: Face {face_idx} - nonzero landmarks: {landmarks_nonzero}, min: {landmarks_min:.4f}, max: {landmarks_max:.4f}")
                            
                            # Check if landmarks are valid (non-zero)
                            if np.any(landmarks):
                                # Landmarks from GPU pipeline are now correctly in original camera frame coordinates
                                # after coordinate transformation fixes - scale them to match display frame size
                                landmarks_display = landmarks.copy()
                                
                                # Debug coordinate ranges before scaling
                                x_range = f"{np.min(landmarks[:, 0]):.1f}-{np.max(landmarks[:, 0]):.1f}"
                                y_range = f"{np.min(landmarks[:, 1]):.1f}-{np.max(landmarks[:, 1]):.1f}"
                                logger.debug(f"DEBUG: Face {face_idx} landmark ranges in frame coords - X: {x_range}, Y: {y_range}")
                                
                                # Get coordinate system info
                                capture_w, capture_h = resolution[0], resolution[1]  # Original camera resolution
                                preview_w, preview_h = 960, 540  # Fixed preview size from gpu_pipeline.py
                                display_w, display_h = display_frame.shape[1], display_frame.shape[0]  # Current display frame size
                                
                                # Fixed transformation: landmarks are in capture space, transform directly to display space
                                # Direct scaling from capture to display (eliminates coordinate mismatch and reduces jitter)
                                scale_x = display_w / capture_w  # Direct scaling from capture to display
                                scale_y = display_h / capture_h  # Direct scaling from capture to display
                                
                                landmarks_display = landmarks.copy()
                                landmarks_display[:, 0] = landmarks[:, 0] * scale_x
                                landmarks_display[:, 1] = landmarks[:, 1] * scale_y
                                
                                # Debug coordinate ranges after scaling
                                x_scaled_range = f"{np.min(landmarks_display[:, 0]):.1f}-{np.max(landmarks_display[:, 0]):.1f}"
                                y_scaled_range = f"{np.min(landmarks_display[:, 1]):.1f}-{np.max(landmarks_display[:, 1]):.1f}"
                                logger.debug(f"DEBUG: Face {face_idx} landmark ranges after scaling - X: {x_scaled_range}, Y: {y_scaled_range}")
                                logger.debug(f"DEBUG: Scale factors - X: {scale_x:.3f}, Y: {scale_y:.3f}")
                                logger.debug(f"DEBUG: Capture: {capture_w}x{capture_h} â†’ Display: {display_w}x{display_h}")
                                
                                # Clamp coordinates to display bounds to prevent drawing outside frame
                                landmarks_display[:, 0] = np.clip(landmarks_display[:, 0], 0, display_w - 1)
                                landmarks_display[:, 1] = np.clip(landmarks_display[:, 1], 0, display_h - 1)
                                
                                # Apply stabilization to reduce jitter
                                landmarks_display = self.stabilize_landmarks(face_idx, landmarks_display)
                                
                                # Draw bounding box around landmarks for debug
                                if landmarks_display.shape[0] > 0:
                                    x_coords = landmarks_display[:, 0]
                                    y_coords = landmarks_display[:, 1]
                                    x_min, x_max = int(np.min(x_coords)), int(np.max(x_coords))
                                    y_min, y_max = int(np.min(y_coords)), int(np.max(y_coords))
                                    
                                    # Draw bounding box
                                    cv2.rectangle(display_frame, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)
                                    cv2.putText(display_frame, f"Face {face_idx}", (x_min, y_min-10), 
                                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)
                                    
                                    logger.debug(f"DEBUG: Face {face_idx} bounding box: ({x_min}, {y_min}) to ({x_max}, {y_max})")
                                
                                # Draw face mesh
                                draw_time = self.draw_face_mesh(display_frame, landmarks_display)
                                self.metrics.draw_time_ms = draw_time
                                logger.debug(f"DEBUG: Face {face_idx} mesh drawing completed in {draw_time:.2f}ms")
                            else:
                                logger.debug(f"DEBUG: Face {face_idx} has no valid landmarks (all zeros)")
                    else:
                        logger.debug("DEBUG: No faces detected in landmark data")
                    
                    # Mark landmark data as read
                    landmark_metadata['ready'] = 0
                else:
                    logger.debug(f"DEBUG: Landmark data not ready - ready: {landmark_metadata.get('ready', 'N/A')}, frame_id: {landmark_metadata.get('frame_id', 'N/A')} vs preview: {preview_metadata.get('frame_id', 'N/A')}")
                
                # Update performance metrics
                current_time = time.time()
                frame_time = (current_time - self.last_frame_time) * 1000
                self.metrics.frame_time_history.append(frame_time)
                self.metrics.fps_history.append(1000.0 / frame_time if frame_time > 0 else 0)
                self.metrics.total_frames = frame_count
                self.metrics.update_averages()
                self.last_frame_time = current_time
                
                # Draw metrics overlay
                self.draw_metrics(display_frame)
                
                # Display frame
                cv2.imshow(self.window_name, display_frame)
                
                # Handle keyboard input
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    logger.info("User requested quit")
                    self.running = False
                elif key == ord('p'):
                    # Toggle pause
                    control_queue.put({'command': 'pause'})
                    logger.info("Toggled pause")
                elif key == ord('s'):
                    # Save screenshot
                    filename = f"gpu_pipeline_test_{int(time.time())}.png"
                    cv2.imwrite(filename, display_frame)
                    logger.info(f"Saved screenshot: {filename}")
                
                frame_count += 1
                
                # Log progress every 5 seconds
                if frame_count % (self.target_fps * 5) == 0:
                    elapsed = time.time() - start_time
                    logger.info(f"Progress: {elapsed:.1f}s, {frame_count} frames, "
                              f"{self.metrics.fps:.1f} FPS, "
                              f"{self.metrics.detected_faces} faces")
            
            # Final statistics
            total_time = time.time() - start_time
            logger.info(f"\n{'='*60}")
            logger.info(f"Visual Test Complete - Final Statistics:")
            logger.info(f"  Total Time: {total_time:.1f}s")
            logger.info(f"  Total Frames: {self.metrics.total_frames}")
            logger.info(f"  Average FPS: {self.metrics.fps:.1f}")
            logger.info(f"  Average Frame Time: {self.metrics.frame_time_ms:.1f}ms")
            logger.info(f"  Average Detection Time: {self.metrics.detection_time_ms:.1f}ms")
            logger.info(f"  Average Landmark Time: {self.metrics.landmark_time_ms:.1f}ms")
            logger.info(f"  Average Draw Time: {self.metrics.draw_time_ms:.1f}ms")
            logger.info(f"{'='*60}\n")
            
        except Exception as e:
            logger.error(f"Test error: {e}", exc_info=True)
            
        finally:
            # Cleanup
            logger.info("Cleaning up...")
            control_queue.put({'command': 'stop'})
            camera_worker.join(timeout=5)
            if camera_worker.is_alive():
                logger.warning("Force terminating worker process")
                camera_worker.terminate()
                camera_worker.join()
            
            self.cleanup()
            logger.info("Cleanup complete")

def signal_handler(sig, frame):
    """Handle Ctrl+C gracefully"""
    logger.info("Interrupt received, shutting down...")
    sys.exit(0)

def main():
    """Main entry point"""
    signal.signal(signal.SIGINT, signal_handler)
    
    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description='Visual test for GPU pipeline with face mesh overlay')
    parser.add_argument('--camera', type=int, default=0, help='Camera ID (default: 0)')
    parser.add_argument('--width', type=int, default=1920, help='Target width (default: 1920)')
    parser.add_argument('--height', type=int, default=1080, help='Target height (default: 1080)')
    parser.add_argument('--fps', type=int, default=30, help='Target FPS (default: 30)')
    parser.add_argument('--duration', type=int, default=60, help='Test duration in seconds (default: 60)')
    args = parser.parse_args()
    
    # Run test
    test = GPUPipelineVisualTest(
        camera_id=args.camera,
        target_resolution=[args.width, args.height],
        target_fps=args.fps
    )
    
    try:
        test.run(duration=args.duration)
    except KeyboardInterrupt:
        logger.info("Test interrupted by user")
    except Exception as e:
        logger.error(f"Test failed: {e}", exc_info=True)
        sys.exit(1)

if __name__ == '__main__':
    main()